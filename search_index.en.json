[{"url":"https://karldyson.github.io/karld-blog-website/posts/","title":"Posts","description":"","body":""},{"url":"https://karldyson.github.io/karld-blog-website/posts/possible-death-of-the-synology/","title":"Death of the Synology...?","description":"...I think there's a strong chance that my Synology is dead...","body":"I had a text a couple of weeks before Christmas from a friend who lives at the other end of the village, asking if the power was out.\nIt wasn’t for me, but a huge area the other end of the village was out and was due to be out for hours.\nSo, they came to visit for a few hours.\nAnyway.\nIn the recent TimeMachine post I alluded to TimeMachine being one of the first things to replace after the Synology had died, or, at least, failed to come back.\nSomething happened that evening; some sort of power glitch. The lights didn’t go out or flicker, but some/most/all ? of the stuff connected to the UPS in the garage seems to have rebooted at the same time.\n…and thankfully everything seems to have survivedi… …apart from the Synology.\nIt’s a DS1812+ and my inbox suggests I originally ordered it on January 10th 2013, so I guess it’s had a fairly good innings.\nMostly, it’s a secondary storage for data and not the primary location for anything so it’s more an inconvenience than anything.\nGoogle, and a friend, suggest that it sitting there failing to spin up the drives whilst stubornly flashing it’s little blue light means that it’s likely unhappy rather than dead, so now that Christmas and New Year are behind us, I’ll open it up and see if any of the obvious gotchas are to blame.\nSearching suggests the first places to check are:\n\nBIOS battery\nDisk on Module (DOM, internal storage for its operating system)\nPSU\nFailing RAM\n\nSince electricity prices soared a few years ago now, and I switched off the lovely, but very thirsty servers running my virtual machines, anything that I want to run home here typically runs on some flavour of Raspberry Pi. They’re small, do the job, and when it comes to electricity, they are quite the opposite, even in what is now becoming a small group, compared to a Dell R720XD full of 15k SAS drives.\nThe oldest of the Raspberry Pi’s are getting long in the tooth and so I’m working on replacing a couple of them.\nAs mentioned elsewhere, the oldest, I think, is the one doing the old temperature monitoring, currently being replaced with a collection of Pico 2W units. There are other posts here that talk about that.\nThe next oldest two are a general workhorse RPi4 that kinda does some monitoring, is the auth DNS server for my internal DNS zone, and the RPiB that does NTP. I wrote recently about the new stratum zero time server\nWith the possible death of the Synology, I wanted to take time to think about whether I need a NAS, or whether other solutions would work better for my needs, and so in the mean time, I got on with replacing the workhorse and the time server in one go, adding a bunch of extra storage for TimeMachine.\nAs you’ll know, I’m a bit of a DNS geek, and I also have DNS related project underway that will gradually use some of the storage (circa 3GB per day), so I’ll know what the longer term plan is for the Synology before I need to work out whether I need to put more storage in the new RPi.\nSorry if a search engine sent you this way hoping for an answer; I’m trying to be better (or at least less bad) at writing these things up, and so I’ll come back and write up what I find soon…\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/stratum-zero-ntp-server/","title":"Stratum Zero NTP Server","description":"Build of the replacement for my aging stratum 0 NTP server","body":"Introduction\nA looooong time ago I made an NTP stratum 0 NTP server out of a Raspberry Pi B and a suitable Adafruit GPS hat.\nIt doesn’t do any of these fancy new GNSS1, it just does GPS2.\nBut it does time and it maintains time sufficiently well, sufficiently accurately and sufficiently stable and consistently that it’s in the NTP.org pool.\nBut a few weeks ago, my Synology died (I’ll write about that soon), and rather than knee jerk replace the NAS, I decided to accelerate the plan to replace a couple of the older Raspberry Pi’s here which would buy me time to consider what to do with the Synology after I got a chance to look at it now Christmas and New Year are out of the way.\nThe oldest RPi is the one doing the original temperature monitoring but replacement of that is well under way, with a few articles talking about what I’m doing there.\nThe next oldest is the RPiB doing NTP and the next oldest is one of the general workhorses, a Raspberry Pi 4.\nThis article covers (what there is of) the build of a Raspberry Pi 5, and the config of NTP etc.\nHardware\nOrder\nI ordered the following parts:\n\nRaspberry Pi 5 8GB\nRaspberry Pi 27W PSU\nPimoroni NVMe Base Duo\nRaspberry Pi Active Cooler\nWaveshare LC29H(AA)\nWD-Black 2TB M.2 NVMe 2230\n\nI conveniently already had a spare 256GB Sandisk microSDXC card which I installed the latest Raspbian on while I waited for my orders to arrive.\nI doubt this unit is going to work particularly hard but for the tiny cost, why wouldn’t you have the cooler fan?\nLastly, the selection of NVMe…. the PCIe bus on the RPi5 does Gen2 speeds by default, and it wasn’t clear whether the Pimoroni board supported Gen3 speeds. So, the 2230 2TB NVMe, a Gen4 device, will never get pushed to its claimed 5,150MB/s, and so being a sensible price, reputable brand, and available, it fit the bill.\nEvery time a buy and build a little Raspberry Pi I consider other little NUC type boxes, but for “a little computer doing nothing particularly CPU taxing and being nice and frugal on the electricity” these things are hard to beat.\nThat whole list came to £335, plus ~£25-30 to cover the cost of the microSDXC I already had (although the Pi will boot from the NVMe perfectly happily).\nBuild\nThe NVMe slots are difficult to reach and doing the retaining screw up would be impossible post build, so I started there, installing the NVMe into the Base Duo, and installing the standoffs and the little rubber feet. If I want to add a further NVMe in future, I’ll need to undo the standoffs from underneath, but that’s fine.\n\n\n\nGetting the PCIe cable installed is fiddly, shall we say. I found it easiest to install the RPi end first and then pop the other end into the Base Duo. If you have ways to hold the two boards still next to each other with the RPi inverted, whilst simultaneously inserting the cable, that might be easier.\n\n\n\n\n\nI then carefully attached the active cooler and plugged in its cable. Be careful here; the plug only goes in one way round. No excessive force is required. No guarantees the cables are always the same colour, I guess, but on all of mine the yellow cable is outermost.\nThe Waveshare GNSS hat attaches to the GPIO pins. I ordered one of the extra tall GPIO headers as well as a 90° angled header so that I could see which worked best once I had the parts laid out on the desk in front of me.\nAs you can see in the pictures, I opted for the 90° header as it ensures the active cooler is completely clear, at the expense of making putting this in a case harder.\nThe Waveshare comes with a weatherproof antenna and the adaptor to connect to the board, so you’re all ready to go when it arrives.\n\n\n\nPictures showing the hardware build\n\n\n\n\nLastly, very carefully connect the GPS antenna to the Waveshare card:\n…it just gently snaps together.\nMake sure it’s located centrally and gently squeeze.\nTroubleshooting / Status\nThere are four LEDs on the board, as follows, starting from the antenna end of the board:\n\nRed - Indicates power is on\nGreen - Pulse Per Second (PPS)\nGreen - Data Received (Rx) - data received by the board from the Pi\nGreen - Data Transmit (Tx) - data transmitted by the board to the Pi\n\nIn normal operations, the red LED should be on, the PPS LED should flash once per second as soon as the board has a 3D position lock and the last LED will flash once per second to indicate the NMEA + PPS data being sent to the Pi.\nIf you’re watching the NMEA output from the Waveshare using cgps or gpsmon you’ll notice that each line of output starts with a five characters preceeded by a $.\nThe format for $XXYYY is as follows:\nXX - tells you the GNSS constellation “talker identifier”\nXXConstellationRegion\nGAGalileoEuropean\nGBBeiDouChinese\nGLGLONASSRussia\nGPGPSUSA\nGQQZSSJapan\nGINavICIndia\nGNGenericAll/Any\n\nYYY - tells you the sentence / protocol identier. There are many, but for example:\nYYYDescription\nGGAGlobal Positioning System Fix Data\nGLLGeographic position , longitude, latitude and time\n\nSoftware &amp; Configuration\nRaspbian installation is very straight forward; you download the Raspberry Pi Imager for your operating system, and pretty much follow the prompts, either picking the desktop version or the lite version (without all the desktop stuff). I’m not going down the rabbit hole of the detail of that, but shout if you think I should do an article on that…?\nThe packages we need here are the GPSD daemon, a time server daemon and optionally some troubleshooting tools.\nBut first, we need to make an alteration to the Raspberry Pi configuration.\nThe Waveshare spec sheet shows us that the PPS signal will be on GPIO PIN 18 and so we’ll add the following to /boot/firmware/config.txt:\n\nWe also need to ensure that:\n\nSPI is enabled\nSerial Port is enabled\nSerial Console is disabled\n\nIt’s a lot clearer in the GUI, but I use the minimal Raspbian because this isn’t going to be plugged into a screen.\n\nsudo raspi-config\n\nGo into the Interfaces menu\nSelect Serial\nSelect it and turn off\nSelect “No” to serial console\nSelect “Yes” to serial port\n\nNow you can reboot your Pi and we can get to configuring the software.\nPackages\nFirst, we’ll install the packages we need:\ngpsd chrony gpsd-tools pps-tools\nNext we can get to configuration:\nGPSD\nMy /etc/default/gpsd looks like this:\n\n…if you then restart gpsd with sudo systemctl restart gpsd then we can check it’s all working\ngpsmon and/or cgps should stream NMEA output and show you the status of your GNSS. It may take a few minutes to get its first lock, so be patient.\nChrony\nTo get NTP onto the network, we need a time daemon. I prefer chrony but I’ve had success with ntpd in the past too if you prefer or are more familiar with that.\nGPSD isn’t a source in Chrony terms. Chrony only expects to find peer, pool and server statements in the .sources files in /etc/chrony/sources.d.\nGPSD is a refclock and so we’ll create /etc/chrony/conf.d/pps.conf with the following contents:\n\nIf we restart chrony we can then see some stats. I’ve snipped the other pool members from the followin output:\n\n\nI’m also going to add this to NTP Pool Project, so I need to permit the internet to come and get time from me, so I also added /etc/chrony/conf.d/acl.conf that looks like this:\n\n…obviously you should set that to whatever is appropriate for your use case.\nIf you want to add to the pool, go to the link above, create an account and off you go. The pool will monitor you and only let you in if you’re accurate and stable.\nMy server has been up for 24 days now. I added to my pre-existing pool account and it’s currently serving ~4,100 clients. Roughly 3,850 IPv4 and 250 IPv6.\nLastly…\nJust make sure the daemons are set to start at system boot:\nsudo systemctl enable gpsd chrony\nFootnotes\n\n\n\nGlobal Positioning System (GPS) is actually the name of the American GNSS, although because it was the only GNSS for a long time, it’s what a lot of people use as the generic term. ↩\n\n\nGlobal Navigation Satellite System (GNSS) is the generic name. ↩\n\n\n\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/timemachine/","title":"TimeMachine","description":"...after the Synology died, one of the things that needed replacing was TimeMachine...","body":"Introduction\nI have a MacBook and whilst some of the things I care about get dropped in iCloud and magic’d away onto someone else’s computer for safe keeping and sync’ing to my other Apple devices, I like to back it up too. The Synology does (did?) TimeMachine and so the first thing to replace was that.\nI googled and read a bunch of articles, and installed samba and avahi-daemon.\nI added the following to the bottom of /etc/samba/smb.conf\n\n…and I set my samba password with sudo smbpasswd -a my_username\nNow, this RPi is in a different VLAN to my main client LAN, and so whilst I’m sure the Mac would spot this in Finder automagically if they were in the same VLAN, or, if I was bouncing mDNS to and from the network services VLAN, ⌘ Command + K, typing smb://the.fqdn.of.the.rpi into the address bar and hitting Connect worked just fine (after adding the appropriate firewall configuration to permit tcp/445 from the Mac to the RPi.\nI authenticated and then opened the TimeMachine control panel. The rest worked just as you’d expect, and a backup kicked off soon after.\n…the SRX config…\nJust in case you’re wondering; this is the slightly redacted config for the SRX…\n\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/temperature-monitoring-part-3/","title":"Temperature Monitoring - Part 3","description":"Improving the temperature monitoring with multicast, prometheus and grafana","body":"Introduction\nFor the moment, I’m still sending updates into IOT Plotter, but I wanted to have more flexibility (and I wanted to tinker some more, honestly).\nPico Changes\nSo there have been a number of changes, additions, tidying and refactoring to the code running on the pico.\nMulticast\nNow, on each polling run, for each temperature sensor, the pico sends a UDP multicast packet onto the network containing a JSON string. The string contains the sensor name and the current temperature.\nOne UDP packet for each sensor for each poll of the sensors.\nWhat this means is that anything else around the network that is interested in the various temperatures, can subscribe to the multicast group and do as it wishes with what it receives.\nThis involves two bits of code, first a subroutine to send a multicast packet:\n\n…and then later, we build a dictionary to pass in and then pass it into that subroutine…\n\nOnboard LED\nI had the onboard LED come on at the start of a polling cycle and go off at the end, just before the sleep. This gives at least a little bit of status without needing to hook up to the computer.\nFirst import the library and intitialise the pin…\n\n…and then in the main loop we can switch the LED on and off…\n\nAPI Call Update\nI also updated the bit of code that makes the API call, and had it send a multicast packet with a status update in it so there’s also an ability to do some remote debug without the need to hook up Thonny being the first port of call.\nThis builds the message_dict within try or except and then passes it to the multicast subroutine in finally to send it.\nPrometheus &amp; Grafana\nI already use Prometheus elsewhere for collecting metrics from various things, but in all cases I’m using existing exporters and predefined Grafana dashboards. This was an opportunity to have a go from scratch. Learn. Understand.\nPrometheus Exporter\nI wrote a basic exporter using the Python prometheus_client library.\nOn my Debian box1 this is as simple as:\nsudo apt install python3-prometheus-client\nThe idea being that the exporter would:\n\nRun a HTTP server on a port so that Prometheus can scrape the metric(s)\nSubscribe to the multicast group and listen for the UDP packets\nUpdate the metric(s) with the current temperature\n\nThe full code can be seen in the Github repository.\nWhilst the code only processes the UDP packets containing packets containing temperature updates, it does log the status updates as well, and so the log file for the running service can be used to debug the current state of the IOT Plotter API calls too.\nIt’d be fairly trivial to alter both the pico code and the Prometheus exporter to talk unicast one to the other, in the event that your network hampers multicast, for example. I may add this to the TODO list.\nInitialise the socket\nFirst, we set up the socket and subscribe to the multicast group\n\nStart the HTTP server\nThen we start the HTTP server that Prometheus will talk to\n\nInitialise the metrics\nThen initialise the metrics to be scraped\n\nWait for packets\n…before looping round handling the packets\n\nFriendlier Sensor Names\nEach temperature update includes the sensor name from the configuration file on the pico. I had ensured that all of the names were formed with a capital for each new word, so Loft, LivingRoom, ColdWaterTank etc.\nIn the exporter, I convert these into “friendly names” that are included in the updates in a friendly_name label, so the above become Loft, “Living Room”, “Cold Water Tank”.\nYou can see the line that acheives this in the highlighted line of code above.\nPrometheus\nConfiguring Prometheus is the usual addition of a job to scrape the target. I’m running Debian, so my config file is in /etc/prometheus/prometheus.yml and I added the following:\n\nGrafana\nIn Grafana I created a new dashboard, and a variable picker. This picker uses the friendly_name field introduced above:\n\n\nScreenshot from Grafana for friendly_name variable definition\n\nThe main graph then uses this, defaulting to “All”.\nIn the following example you can see:\n\nwhen the heating comes on, whether it’s feeding the hot water, radiators or both\nthe temperature in the loft, and the water in the cold water tank\n\n\n\nScreenshot from Grafana of temperature graphs\n\nAlerts\nI created an alert in Grafana that will tell me if the water in the cold water tank gets to 3° celsius so I know if it’s approaching freezing (although its a reasonable volume so won’t freeze quickly)\nI also wanted to be alerted as to whether updates had stopped arriving for a sensor.\nI looked at a combination of last_over_time and changes, but, particularly with the cold water tank, the temperature could be very stable (and not change in value) for hours on end.\nSo I decided to add a timestamp metric that is updated at the same time as the temperature. So, even if the temperature value doesn’t change, the timestamp, which is the unixtime value at the time of the update, will always change.\nThat allowed me to create an alert for a sensor on each pico (bearing in mind that some picos have multiple sensors, I don’t want to be alerted multiple times for each pico)\n\nSummary\nA nice thing about the design is that if you configure and plug in a new sensor:\n\nIt’ll start multicasting temperature updates to the network.\nThe prometheus exporter will pick the updates up automatically and include them for scraping.\nPrometheus will pick them up automatically the next time it scrapes.\nFinally, Grafana will see them automatically, and at least for the main graph on the dashboard, they’ll appear automatically.\n\nFootnotes\n\n\n\nok, this is actually a Raspberry Pi 5 running Raspbian, so it’s the ARM64 (aarch64) version of Bookworm at the time of writing ↩\n\n\n\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/hikvision-frigate-homeassistant/","title":"Hikvision, Frigate and Home Assistant","description":"Getting the third stream working on the Hikvision cameras and then plumbing that into Frigate","body":"Introduction\nA while back now, I started using Home Assistant and that quickly grew to me adding my first camera and using Frigate.\nMy first camera was a DS-2CD2387G2-LU.\nAt the time, I had thought my camera had a third stream, but I had only managed to persuade Frigate to work with the first, with patchy results with the second. The third didn’t seem to work at all.\nMore recently, I had wanted to add another camera, and bought the current equivalent of my original (DS-2CD2387G3-LI2UY), to remain as consistent as possible.\nThis too apparently had a third stream, but again, I could not persuade it to work.\nWhilst reading something else trying to get audio working (in Safari) for the live feed, I stumbled across a post telling me how to get the third stream working on the cameras.\nSo, to cut to the chase, you need to switch VCA mode to “Monitoring”.\nWhen you save, the camera will reboot and then your third stream will then magically appear.\nI had wanted this so that I could have a lower but still reasonable resolution for detection whilst continuing to record the main 4K stream.\nI hope this is useful to someone else!\nDetails\nSome of this may be useful too, so I’ll chuck it in here for now…\nConfiguration\nIf, like me, you’ve been having a challenge with any of this, then this is the config I’m currently running:\nEach camera is essentially the same, so I’ll just include the config for one.\nThis is all in the /addon_configs/ccab4aaf_frigate-fa/config.yaml file.\nThis first bit has been there since the original camera, and I’m not entirely sure it’s still needed as audio has become a more mainstream feature. But I’ve not looked at that yet, so it’s still there.\n\nNext, go2rtc config:\n\n…and then finally, the camera:\n\nCamera Configuration Screens\nOlder Camera\nFirst, the older of the two, running firmware V5.7.3 build 220112\n\n\nOlder camera config screen showing where to flip RCA Resource to Monitoring\n\n\n\nOlder camera config screen showing the third stream showing up post reboot\n\nNewer Camera\nThen the newer one, running firmware v5.8.10 build 250605 currently\n\n\nNewer camera config screen showing where to flip RCA Resource to Monitoring\n\n\n\nNewer camera config screen showing the third stream showing up post reboot\n\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/temperature-monitoring-part-2/","title":"Temperature Monitoring - Part 2","description":"Implementing the new temperature probes to monitor the heating system","body":"Introduction\nFollowing on from the previous post where I wrote about an updated approach to monitoring temperatures around the house, I made some updates and implemented monitoring the various temperatures related to the boiler, hot water and radiators.\nUsing the board designed and tested in Temperature Monitoring – Part&nbsp;One, I picked up some DS18B20 sensors on longer wires and with waterproof ends.\nSensor Locations\nThey were run to the following locations:\n\nThrough the loft to the cold water tank where it’s lowered until it almost but not quite touches the bottom of the tank.\nIn the loft at approximately the height of the top of the cold water tank\nIn the airing cupboard tied to the feed from the boiler to the valve\nIn the airing cupboard tied to the feed from the valve to the hot water tank\nIn the airing cupboard tied to the feed from the value to the radiators\n\nThe sensors come with a metal housing on the end, and so I made a makeshift strip from kitchen aluminium foil; about an inch wide and as long as the width of the roll. I wrapped this around the pipe once, and then included the sensor housing, before wrapping the rest of the tape around. I fastened with a couple of cable ties.\n\n\nSensor cable tied to pipe with makeshift aluminium foil wrap\n\nDue to the short distances in the airing cupboard, I placed the hot water and radiator sensors as far from the valve as possible to try and minimise the pick up of temperature conducted through the pipe itself, trying to minimise reading temperature on the radiator sensor when the water is being directed to the hot water tank, and vice versa.\nThere’s a 3 pin socket in the airing cupboard and so I replaced the faceplate with a socket that included a USB charging socket. This allowed me to connect the Raspberry Pi board up in there.\nThe sensors were connected to the board and the software loaded.\n\n\nRaspberry Pi Pico 2W soldered to the board and connected to the sensors\n\nYou’ll have to ignore the state of the airing cupboard!\nYou can see in the top left, the feed from the 3 way valve into the hot water tank, and in the lower right you can just about see the sensor on the feed into the valve, and the sensor on the pipe just before it disappears into the floor to feed the radiators.\n\n\nAiring cupboard with sensors tied to pipework\n\nIssues\nI noticed that the updates to the IOTPlotter API would stop, and require the pico to be restarted. I did some troubleshooting by running with Thonny connected via a long USB cable.\nThe call to the API would periodically have issues, either due to a brief outage of my broadband line, or (I suspect) possible maintenance of the API, etc. The code had not included a timeout on the API call, so I introduced one along with a try, except.\nI think there is also an issue whereby periodically the code gets tripped up while polling the sensors but I’ve not caught that yet.\nThis reduced the issues but ultimately I had another idea to improve things, coming soon in part 3…\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/temperature-monitoring-part-1/","title":"Temperature Monitoring - Part 1","description":"Part 1 in a series of replacing my temperature monitoring","body":"Introduction\nI’ve written about monitoring temperature around the house before.\nThe solution I wrote about then is still working well.\nBut, more recently, I wanted to not only update the system, play with some new things, but also a minor underlying driver was the desire to be able to see when the boiler was on, and where the heat was being sent; radiators and/or hot water.\nI had also been looking for a reason to have a play with Raspberry Pi Pico and I’d found that it’s fairly trivial to use DS18B20 sensors on one of the Pico’s GPIO pins.\nHardware\nSo, I placed an order for:\n\na Pico2W\na small breadboard to prototype it\na solderable breadboard for the final item\nand some DS18B20 sensors\n\nThe DS18B20 has a low power requirement, and so with the Pico powered by a suitable power supply, 3V3 can be taken from pin 36, with ground on any of the GND pins. I used pin 38 in reality, but pin 28 makes the diagram tidier.\nThe data pin from the DS18B20 can be connected to any GPIO pin, with the 4k7 resistor then connected between data on the GPIO pin and 3V3.\nEach DS18B20 has a unique 64 bit address, and so it’s possible to connect multiple sensors to the same GPIO pin.\n\n...and prototyped on breadboard...\n\n\n\n\nCircuit Diagram\n\nSoftware\nNext we need to write some code for the microcontroller. Of the languages supported by the Pico, I am most familiar with Python, so I grabbed a MicroPython image1.\nWhen first connected to your USB port, a new Pico presents in bootloader mode. Loading the MicroPython image is as simple as copying the U2F image file to the presented volume upon which it will automatically reboot.\nYou can persuade the Pico to connect in bootloader mode in future by pressing the bootloader button while you connect the power.\nI use Thonny for programming the Pico.\nWe need the code to do a few things:\n\nconnect to the WiFi\ninitialise the temperature sensor(s)\nsend the collected temperature data somewhere\n\nA friend was using iotplotter.com to send time series data and visualise it in a graph so I decided to use that as the API is nice and simple to proof-of-concept.\nA few articles referenced the itk_pico code, and that gave me a bit of a head start too.\nI’ve forked it in order to make a couple of tweaks (see below); you can find it here which also references back to the original authors work/repository.\nUnless I’m mistaken (I’m new-ish to Python, so this is likely!) the code assumes a single connected temperature sensor, as it returns from the subroutine as soon as it gets a temperature. For my use case, I want to be able to have multiple sensors. I also wanted to be able to give the sensors ascii names, so I made some modifications to the temperature.py code…\nFirst, a method to convert to a human friendly name:\n\n…a method to return all of the friendly names…\n\n…modified the get_temperature method to loop around all sensors and return all of the temperatures…\n\n…and lastly, some tweaks to the initialisation; a bit more debug output.\n\nWhen the pico is powered on without a console, ie: just plugged into a USB PSU rather than your computer’s USB port, it executes main.py so lets have a look at that…\nI’m only using the temperature, wifi and logger code, so we import those. The IOT Plotter API expects the payload as JSON, we have our config, and we’ll use the requests module to POST to the API.\n\nNext, we’ll initialise the temperature sensors and friendly names…\n\nWe’ll initialise default values for each sensor and then have a look in the config to see if there’s a specific setting for each…\n\nThe main loop looks like this. It’s possible to set the time for each data value but we’re sending data directly in real time, so by omitting the time, IOT Plotter will use the current time.\n\nThe config file looks like this:\n\nThe full code can be found in the github repo.\nHaving tested it, I soldered the board up.\n\n\nThe Pico and circuit soldered up\n\nSensors can be connected to the screw terminal on the top right.\nStay tuned for the next article on implementing this…\nFootnotes\n\n\n\nMicroPython for Raspberry Pi Pico 2W ↩\n\n\n\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/things-ipv6-mostly-breaks-part-1/","title":"Things IPv6 Mostly Breaks, Part 1","description":"Things that appear to be broken by IPv6 mostly networks","body":"Following my recent article on ipv6-mostly, last weekend I enabled this on my main network segment as it had all gone well on the guest network.\nI had half expected a bunch of things to stop working in some way…\nWould the Hue app on my phone still talk to the Hue bridge?\nWould the Alexa app on my phone still talk to the devices in my home?\nI assumed a bunch of these things probably all make connections to a cloud service and communicate between themselves via that.\nNothing appeared to break.\nUntil I tried to cycle on Zwift.\nZwift seems to work fine, however, the companion app doesn’t work as an in-game companion. It talks to Zwift all ok, shows events, activities, etc, just no companion.\nI suspected that the game communicates its local/internal IP address to Zwift’s servers, and then the companion app gets the IP from there and makes a local connection direct to the game.\nHowever, the local IP that the game will find on the interface is an unroutable IPv4 address between 192.0.0.2 and 192.0.0.7 from the DS-Lite reserved IP addresses (see RFC6333)\nSo, if the game communicates this via Zwift servers, the companion app is going to fail to make a connection to that.\nI manually added a valid IPv4 address to both the Apple TV where the game is running, and my iPhone, and (with a game exit &amp; restart) the companion started working just fine.\nI’ve raise a support ticket with Zwift. I’ll let you know…\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/ipv6-privacy-extensions-linux/","title":"IPv6 Privacy Extensions on Linux","description":"How to make IPv6 privacy extensions work on Linux","body":"Introduction\nThis is definitely one of the blog posts written to remind me how to do something.\nI’m sure I knew this, but returning to it on a new box caused me frustration that Googling did not help with. Maybe my google-fu is broken?\nIt was only after quite a bit of googling and reading things that I spotted something in a comment that reminded me of the thing I was missing.\nSo, I’m writing this so that it will remind me in future, and may help others.\nScenario\nI had built a new Raspberry Pi 5 as a pi-hole server.\nOf course, in order to ssh to it and browse to the admin UI, it has fixed IP addresses.\nActual DNS service is done by advertising service IP(s) into the router’s routing table so I can do some ECMP and resilience.\nI wanted it to recurse to the internet from privacy extensions IPs so that the recursive source IPs change over time.\nI knew I needed to stick a 2 in /proc/sys/net/ipv6/conf/eth0/use_tempaddr and that things worked best if you also stick 1 in /proc/sys/net/ipv6/conf/eth0/accept_ra\nBut, nothing. No dynamic IPv6 addresses.\nBother.\nI knew there was something else I was pretty sure there’s a third file I need to do something with, but could I remember which one it was?\nNope.\nSolution\nSo, I’ll cut to the chase.\nFor automatic IP configuration, there also needs to be a 1 in /proc/sys/net/ipv6/conf/eth0/autoconf\nConfiguration\nThis is what I have done to fix this such that it’s reboot and upgrade safe. By all means leave me a comment below if I’ve missed something obvious here!\nsysctl can be persuaded to make these changes as follows:\n\nI don’t use NetworkManager for my eth0 IP addresses, as I’m more familiar with /etc/network/interfaces so this is my static config and a poke to make sure that eth0 picks up the settings too.\n\nFixed!\n…and this is what the output looks like now that that is working…\n\n..and remote things on the internet see the correct source for my outbound connections…\n\n\nTechnicalities\nuse_tempaddr\nValues in this file have the following meanings:\n\n&lt;= 0 : Privacy Extensions is disabled\n== 1 : Privacy Extensions is enabled, but public addresses are preferred over temporary addresses\n&gt; 1 : Privacy Extensions is enabled, but temporary addresses are preferred over public addresses\n\nNote that in some very brief and not very scientific testing, if autoconf is enabled and use_tempaddr is set to 1, the addresses that seem to be preferred are the autoconf addresses even if you have a static configured. I don’t think this surprises me, but worth noting.\naccept_ra\nSimply put, 0 to disable, 1 to enable the acceptance of router advertisements.\nNote that if you disable this, you will need to specify the gateway statically if you have not already.\nIf this is disabled but use_tempaddr is non zero, temporary addresses can still be created but just not using prefix information from the router advertisements.\nautoconf\nAs above; 0 to disable, 1 to enable the automatic configuration of IP addresses.\nIf accept_ra is set to 1 and this is set to 0, router advertisements will be used for the gateway but not for IP address configuration locally.\nIf this is enabled and use_tempaddr is not, stateless (SLAAC) addresses will be added to the interface but no privacy extensions addresses.\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/ipv6-mostly/","title":"IPv6, Mostly...","description":"","body":"\n\t\n\t\tWarning\n\tTL;DR: This article turned out longer than expected, so if you just want the tl;dr, skip to that bit.\n\n\nIntroduction\nIPv4 has run out. This is not news.\nIPv6 uptake, however, has been slow given it was first created in 1995.\nIt can be intimidating, if you’re not a network expert, and there are still things that are perceived to just work best if you’re doing IPv4.\nExamples include getting information about the network to a client, other than the client’s IP address.\nWith IPv4, when you connect to a network, by default in most cases, your device asks the network for an IP address using a protocol called Dynamic Host Configuration Protocol (DHCP).\nBut the protocol doesn’t just give your device an IP address; it can also let your device know about a bunch of other things on the network. Most commonly the DNS servers that will resolve names for you, but also things like NTP servers, proxy discovery, and more can be conveyed to the device using this mechanism.\nSo IPv6 just needs a version of DHCP then? Well sure, and it does, cunningly named DHCPv6. But for lots of reasons, on a client network, you probably don’t want to be handing out IPv6 addresses with DHCPv6. IPv6 is more modern and can sort out its addresses statelessly and automatically using something called SLAAC. You probably want IPv6 to be stateless for a variety of reasons. Privacy is high up the list.\nPrivacy?\nLet’s take a step back.\nWith IPv4, the address space is tiny compared to the number of devices on the modern internet. Think about your phones, iPads, laptops, home assistant, smart speakers; the list goes on. Even a modest modern home has dozens of devices needing an IP address.\nSo, there are blocks of IPv4 addresses reserved for use within private networks that will never be routed on the internet. Those private addresses can be routinely used in many networks concurrently because whilst IP addresses have to be unique, they only have to be unique within a given network.\nYour device gets one of those private addresses.\nBut, how do you communicate on the internet if your device has an IP address that doesn’t work on the internet?\nNetwork Address Translation (NAT)\nWhen your device makes a connection to something on the internet, your router does something called Network Address Translation (NAT).\nThat is to say, on the inside of your network there will be plenty of those private addresses for all of the devices, but your router will map them all onto a much smaller pool of internet routable addresses and then juggle making sure the right internet traffic is sent to the right device.\nThis means that it’s harder for websites to track you using your IP address, because a whole bunch of devices and users will be “hidden” behind a much smaller NAT pool, possibly just a single IP.\nWith IPv6, the size of the address space is vast. There’s no need for NAT and so the IP address your machine has, is the IP that things you connect to on the internet see. For example when you browse a website.\nInstead of all devices on a network being hidden behind one IP, every individual device has a real internet IP address.\nThis makes it a lot easier for websites to use your IP address to track you, if your IP address is now per device.\nSo just how big is IPv6 to not need NAT…?\nWon’t we run out? We did with IPv4 after all…\nIPv4 has a total of 232 IP addresses. That’s 4,294,967,296. Four billion and change. Sounds like a lot, right?\nIPv6 has a total of 2128 IP addresses.\nThat’s 340,282,366,920,938,463,463,374,607,431,768,211,456\n340 trillion trillion trillion.\nFor context, that’s more than 100 times the number of atoms on the surface of the Earth1.\nIt was calculated that we could assign a unique /48 to every human being on Earth for the next 480 years before we would run out.\nA /48 is the expected common assignment block size. It’s what my ISP has assigned me for my home network. It’s the smallest block of IPv6 address space that you can route on the wider internet.\nYour typical individual network subnet will be assigned a /64.\nA /48 contains 65,536 /64 subnets.\nA /64 network contains 18,446,744,073,709,551,616 usable IPv6 addresses.\nBack to Privacy, then…\nSo, IPv6 has the concept of Privacy Extensions (RFC8981) which means that it has a mechanism for your device to assign itself new IP addresses temporarily, use them for a while, and then discard them. But it requires that your network is assigning addresses statelessly.\nRouter Advertisements\nIf you’re doing stateless IPv6 addresses, and you want to move away from using IPv4, how do you tell your devices about things like DNS resolvers?\nIn order to statelessly configure its IPv6 addresses, your device sends something called Router Solicitation (RS) messages to the network. Available routers on the network respond with Router Advertisement (RA) messages.\nThese RAs include the network prefix(es) in use so SLAAC and privacy extensions can work. They can also be used to tell the client device whether to actually use stateless addresses, whether to try DHCPv6, or whether to do a bit of both, and use stateless addresses but with other information (like DNS servers) from DHCPv6.\nMore recently, to avoid the need for DHCPv6 at all, these RAs can now contain the DNS server information (RDNSS).\nOK, so we’ll switch off IPv4 inside our networks? We’ll give all the clients an IPv6 address.\nWe can use the RA to give devices DNS servers.\nBut what about things on the internet that are still only configured for IPv4…?\nHow do you connect to them?\nNAT64\nIn a similar way to how NAT works above, translating networks of private addresses into a smaller pool of internet routable addresses, it’s possible to translate addresses between address families: ie: between IPv4 and IPv6.\nYour router can tell your device that the network has the ability to do NAT64.\nWith this, devices connect to a mapped IPv6 address for any internet device that only has IPv4.\nThere’s what’s called a well-known allocation for NAT64: 64:ff9b::/96.\nThis allows us to map the entire IPv4 address space. For example, let’s say your device wants to connect to a website that only has IPv4 and its address is 198.51.100.189. This maps to 64:ff9b::c633:64bd.\nIf you look carefully at the end of that, you can see we have c633:64bd.\nc6 in hexadecimal is 198 in decimal.\nConverting the rest, then: 33 becomes 51, 64 becomes 100, and bd becomes 189.\nYour device makes an IPv6 connection to 64:ff9b::c633:64bd and when this reaches the router, it knows to translate this back the other way and make an IPv4 connection to 198.51.100.189.\nBut how does the device know to attempt to connect to that mapped IPv6 address?\nDNS64\nOne option is that you give them a DNS server that does DNS64.\nUsually, dual stack devices will prefer IPv6. As such, when they do DNS lookups to get the IP address for whatever the device needs to connect to, they will look up an IPv6 (AAAA) DNS record first, and fall back to IPv4 (A record).\nSo, a DNS64 capable DNS server looks up the AAAA record. If it exists, it gives the response back to the device and everything works over IPv6.\nHowever, if the AAAA record doesn’t exist, the service only supports IPv4, the DNS64 server looks up the IPv4 A record, but instead of telling the requesting device, the DNS64 server generates the relevant mapped NAT64 IP address and returns that in the AAAA record.\nThe device makes what it thinks is an IPv6 connection, and the NAT64 configuration on the router does the translation to IPv4.\nGoogle offer public DNS64 servers on 2001:4860:4860::64 and 2001:4860:4860::6464\nBut, some components that run on your machine can decide to use DNS servers of their own choice. This can lead to some things not using a DNS64 server, not getting the mapped IP, and continuing to use IPv4 which may be undesirable.\nCustomer Translator (CLAT)\nA second method is a customer translator or CLAT.\nYour device learns that the network can do NAT64 from the network itself. It learns what the prefix is and can do the mapping from the desired IPv4 address to IPv6 under the covers within the operating system.\nFurther, your applications don’t need to know that they’re not making an IPv4 connection.\nYour device puts an unroutable placeholder IPv4 address on its network interface to keep up the pretence that IPv4 is working as usual.\nWhen your application attempts to make an IPv4 connection, the CLAT within your device works out the mapping, and makes the IPv6 connection to the mapped IPv6 address. The router does NAT64 and it all works seamlessly.\nThis even works for IP literals.\nDHCP Option 108\nSo, how do we persuade devices that the network supports IPv6, and can do NAT64 for those pesky “IPv4 only” things?\nMany devices still send out DHCP for IPv4 first.\nSo there’s a mechanism for saying “if you support CLAT, can use the network’s NAT64, and don’t want or need an IPv4 DHCP lease”.\nIt’s called “option 108”.\nDevices capable of IPv6 mostly, will set option 108 in their DHCP request. If the DHCP server replies with option 108, the client doesn’t take an IPv4 lease and applies the placeholder/unroutable IPv4 address to the interface.\nSo, lets make it work…\nI wanted to tinker with this and see it in action.\nMy router/firewall was a Juniper SRX240 which sadly will not run a new enough version of Junos to support RDNSS.\nIt didn’t support telling the clients about the NAT64 prefix either, and it seemed very unhappy with me trying to persuade it to do DHCPv6 for just the DNS information.\nSo, this felt like a great excuse to finally replace it, and so now I have a SRX340 in its place.\nThe next thing to consider is client device support. My laptop is a MacBook, my family use iPhones and iPads. There’s the usual plethora of IoT things around the network too.\nYour mileage may vary if you have other things around your network.\nSo I started with the guest VLAN as there’s less in there to disrupt.\nI want stateless addresses. I need to configure router advertisements anyway, so I’d like to skip needing DHCPv6 configured.\nConfiguration Time\nNAT\nFirst we’ll configure NAT64 so that when we tell the clients it’s available, it actually is. We need to do two things here:\n\n\nWe need to take traffic destined for the NAT64 prefix and static NAT it to IPv4. You can see this in the static NAT section at the top of the following configuration snippet.\n\n\nWe need to then source NAT the traffic so that it comes from an internet routable IPv4 address. I’m fortunate here in that I have a /29 from my ISP, and so to aid troubleshooting, I have the NAT64 traffic SNAT from a different IP to the IPv4 native traffic.\n\n\n\nJunos Configuration\n\n\nSLAAC, RDNSS and PREF64\nNext we need to tell the SRX to do SLAAC, RDNSS and PREF64…\nFirst, our interface needs an IPv6 address.\n\nNext, SLAAC, RDNSS and PREF64.\nWithin the protocols router-advertisement section, we will use the following options:\n\ndns-server-address option to set the RDNSS server(s)\nnat-prefix to set the PREF64 prefix\n\n\nOption 108\nLastly, we’ll support clients that support DHCP option 108 in our DHCP configuration.\n\nDid it work?\nThis is from a MacBook running MacOS 15.\nWe can see that the RA contains the configured details.\n\nWe can see that the interface has stateless IPv6 configured, including privacy extensions. It has also enabled CLAT46, and has added the ‘placeholder’ IPv4 address.\n\nThe device thinks it has an IPv4 connection to something in Microsoft…\n\nWhat does the firewall have to say about that….?\n\n…but 52.123.128.14 maps to 64:ff9b::347b:800e so lets check again…\n\nThe IP details have been redacted/sanitised, of course, but note that the SNAT IP is the guest-nat64 pool IP and not the regular guest pool IP.\nThe End!\nWell, that turned out a bit longer than expected and has a couple of tangents along the way. If you stuck around for all of it, well done and thanks.\nI hope it’s been useful or interesting!\nFootnotes\nFurther Reading\n\nhttps://www.ietf.org/archive/id/draft-link-v6ops-6mops-01.html\nhttps://2023.apricot.net/assets/files/APPS314/apnic55-deployingipv_1677492529.pdf\nhttps://blog.apnic.net/2019/06/07/how-to-slaac-dhcpv6-on-juniper-vsrx/\n\n\n\n\nInternet Society IPv6 FAQ ↩\n\n\n\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/nintendo-switch-nat-types/","title":"Nintendo Switch NAT Types","description":"Tinkering with NAT on my firewall to see if I can improve the Nintendo Switch detected NAT type","body":"Like lots of people, my daughter has a Nintendo Switch.\nA few weeks ago she came to me because a game she was trying to play online was complaining about the NAT type.\nSo we had a look in the network connection test and found it was reporting NAT Type D.\nI have a Juniper SRX here (of course) and the Switch was just using the general outbound source NAT that looks a little like this (actual IPs redacted, naturally).\n\nThe pool is just a very basic pool with a simple address specified.\n\nSome googling later and I find that this seems to be because port translation is in use.\nSeems the Nintendo does not like that.\nThe Switch already has a static DHCP lease so it always gets the same IP.\nI’m fortunate to have a /29 from my ISP and I had an IP spare, so I created a new pool for the Nintendo with port translation disabled.\n\nGood news. This gets the NAT Type up to NAT Type B and the game started working.\nBut this got me to thinking. What was needed for NAT Type A…?\nGiven nothing else was using the public IP, I altered the NAT configuration to a static NAT.\n\nThis, however, still results in NAT Type B.\nThere seemed to be two obvious options remaining.\n\n\nType A is actually “No NAT at all”\n\n\nType A is “there’s effectively no firewalling“\n\n\nTesting option 2 was quicker and simpler than 1.\nAs much as I detest any-any type policies, the outbound policy all along for the Switch was a basic “allow the Switch outbound to the internet” policy.\nSo I added an any-any inbound policy permitting anything inbound to the Switch.\nBingo! NAT Type A.\nSo, that’s horrible.\nGiven NAT Type B is good enough for 99 point something percent of things, that any-any inbound policy was disabled as soon as the test completed.\nI will have to have a little faff with the network so that I can drop the Switch into a VLAN where I can give it a public IP directly so there’s actually no NAT at all, and see what it says then.\nI just thought this might be useful to someone.\nThanks for reading.\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/robin/","title":"Robin","description":"Pictures of the Robin frequenting our garden","body":"\n\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/blocking-adverts-tracking-malware-rpz/","title":"Blocking Adverts, Tracking & Malware using RPZ","description":"Blocking adverts, malware, trackers using blocklists and DNS response policy zones","body":"Introduction\nA while back, I decided I wanted to prevent at least some adverts and tracking, but rather than on a device by device basis, I wanted to achieve this for all devices on the network. Those that know me and my recent work will understand that naturally, DNS blocking sprang to mind, as I’m already very familiar with RPZ.\nOriginally, I was consuming a bunch of lists with some code, manipulating the entries with some weighting and then outputting an RPZ for my servers to use. However, more recently I found Energized Protect, which has a load of different levels of blocking, and they provide the different levels in a variety of formats, helpfully including RPZ. So, I’ve been trialling their lists for a couple of weeks now.\nAs with any external feed, you need to be aware of either false positives being added to the list by the curator, as well as things they think should be on the list that you may disagree with. I was recently affected by this with my Amazon devices, whereby one or more domains critical to the correct functioning of the Echo devices found their way onto the block list I’m consuming. To be fair, I’m consuming one of the more extreme variants of the list, and so this was something I was aware could happen (although I admit, it didn’t spring straight to the front of my mind when troubleshooting over the weekend!).\nSo, let’s talk about how RPZ works.\nRPZ is a feature within some DNS servers that allows you to modify the responses given to clients depending on a number of different criteria. BIND from Internet Systems Consortium (ISC) was pretty much first to have RPZ, but others have varying levels of support for the main functionality. The BIND implementation allows you to define a policy that can consist of a number of layers. Within the policy you can override the entire contents of a layer, and within each layer you can have permit and deny actions based on a number of triggers. For this use case, we are interested in two of the triggers:\n\nthe name being looked up\nthe IP of the client making the request\n\nThe file we download from Energized Protect will form the main blocking layer, and we’ll override the entire layer at the policy level with NXDOMAIN. Arguably we could send queries to a web server with a block page, but not all things on the requesting end of this are browsers, and we can get logging from the BIND servers if we want to know what was blocked for a given client for the purposes of troubleshooting. Of course, we will want to be able to override these entries incase something gets on the list that we don’t want to be affected by (see above).\nRPZ layers are DNS zone file format (see RFC1035 section 5 if you’re particularly interested in DNS master zone format, or for RPZ you can read the RFC draft (it’s not made it to a full RFC yet…)).\nBecause they’re DNS zone files, they can be transferred to other DNS servers using the normal notify and transfer mechanisms.\nOn my network here, there’s a central authoritative server, and then a pair of recursive servers that deal with actual client requests. I’ll get around to writing about the anycast set up of those in another article.\nFor the purposes of this article, the authoritative master is on 192.168.1.53, and the two slaves that are actually dealing with the client recursion are on 192.168.1.51 and 192.168.1.52.\nCentral Authoritative Server\nWe’ll start with the central authoritative server. There are two bits to this, periodically fetching the RPZ, and serving it to the slave servers.\nThe Code\nAll of the scripts I talk about below, can be found in the code repository. The code is fairly straight foward, but of course, drop me a line if you have questions.\nEnergized Protect update their feeds every 6 hours, and so there’s no need to poll them any more often than that. Further, the updateblockrpz script keeps an unchanged copy of the downloaded file so that wget can do timestamping and only download the file if it has actually changed on the server.\nThere are two further scripts, both of which allow you to manipulate an override layer in the policy.\nThe first, rpz-override, allows you to add and remove domains from the override, either to add things you want to block, or allow things blocked in the block layer.\nThe second script, rpz-override-client, allows you to base the action on the client IP instead of on the queried name.\nBoth of these are written in Perl, and more specifically are built on the Net::DNS module to send the changes into the server via a dynamic update.\nThe Config\nNext, let’s look at how we configure the server. A base understanding of BIND configuration is assumed.\nFirst, we’ll need to config it to master the two zones, permit dynamic updates on the override zone, and permit slaves to transfer them. Depending on your distro, the location of your named.conf may vary, and also whether it’s a single file or split out with includes. I’ll just include generic config here to try and cover as many bases as possible.\n\nNormal rules apply here; config like also-notify can inherit from the main options section, or can be overridden per zone like we have done here (line 4 to force just the specific entries listed in lines 5-7).\nWe do the same again with the override zone (lines 14 &amp; 15-17), but here we also add the allow-update (line 19), in order to permit the maintenance scripts to work.\nIf your main options section has allow-update specified, you will need to specify allow-update { none; }; in addition for the block zone, to prevent BIND from keeping journals for the zone. If you need other config that will lead to journals, such as ixfr-from-differences, for example, then the updateblockrpz script may need a tweak to freeze and thaw the block zone instead of just reloading the update.\nI run the updateblockrpz script from cron at a randomly selected minute after the hour, every 6 hours and lazily capture the output to a tmp file for troubleshooting purposes. Yes, I should likely update this to log properly!\n\nSlave Servers\nHaving got the RPZ zones set up on the master, we can turn our attention to the slaves that are actually handling the queries from the clients on the network.\nFirst, we’ll slave the RPZ zones from the master:\n\n…and next, we’ll define the policy that’ll apply to the clients:\n\nAs we mentioned before, we’re overriding the block layer at the policy level, forcing anything in that layer to result in a NXDOMAIN response. The override layer is left as given so that the actions in the layer carry. The policy is evaluated top to bottom, with the first action encountered causing an exit from policy, hence the override layer, which could be whitelisting something that’s in the block layer, is listed first.\nRPZ Entries\nLastly, we’ll just briefly cover different types of record that you might want to put in the override layer; the scripts will help you mostly with this, but for those that are interested, here’s a little more detail.\nBroadly, as we discussed earlier, we’re interested in two main triggers; the name being looked up, and the client making the query.\nEntries that affect the domain name being looked up broadly look like this:\n\nWhere &lt;action&gt; is one of the following:\n\nrpz-passthru (whitelist)\nrpz-drop (drop the query - quite unfriendly, will cause the client to wait for a timeout)\n. (a literal dot, which will cause a NXDOMAIN response)\n\nIt’s also possible to do something like this, if you want to override to a block page or honeypot, for example:\n\n…and of course, any of those can be prefixed with *. to cause the action to apply to everything within the bailiwick of some.domain.name.\nEntries that affect the client look a little different. Firstly, they’re reversed, a bit like in-addr.arpa zones but they’re prefixed by an additional item specifying the CIDR notation. So, if you want to (using the actions from above) whitelist all queries from single IP 192.168.58.3, you’d do:\n\nHowever, if you wanted to block the upper /25, you’d do this (note use of the subnet IP, you need to specify the correct subnet boundary IP):\n\nOther Trigger Types\nWe’ve not talked about the other triggers, but briefly, you can also trigger actions based on:\n\nrpz-ip - the IP addresses that are returned in the answer to a query.\nrpz-nsdname - the domain name of the nameservers that are authoritative for the domain in the query.\nrpz-nsip - the IP addresses of the nameservers that are authoritative for the domain in the query (ie: what the names in (2) resolve to).\n\nType 1 can lead to data exfiltration, which, if you’re blocking a domain because you want to prevent exfiltration, defeats the object.\nIf you put type 1 or type 3 in a layer, then if BIND reaches that layer as it works through the policy, it will do the recursion to the authority for the zone in order to work out if the trigger is a match.\nIf you’re worried about data exfiltration, you MUST put the domains you’re blocking for that purpose in a RPZ layer above the first layer that includes type 1 or type 3 entries, then BIND will execute your configured action without any recursion.\n…but what about DNSSEC\nIf you’ve read all that, and you’re thinking to yourself “hey, but surely returning modified answers will break DNSSEC” then you’re right. Your client machine stub-resolvers will trust your DNS resolver, and so won’t notice, but if you’re pointing a validating resolver at this setup, you’ll need to make sure you keep the break-dnssec yes; option I included above. Possibly counter-intuitively, this causes your RPZ server to lie to the downstream validating resolver.\nIf baddomain.com is DNSSEC signed, and is on your block list, the downstream validating resolver will usually be sending queries with CD set instead of trusting your validation, expecting your server to send all the required DS, DNSKEY, etc. with break-dnssec yes; the RPZ server will lie; it’ll pretend baddomain.com isn’t signed and will strip all DNSSEC data in responses to the downstream resolver(s).\nIt’s important to note that this has an edge case. Let’s imagine you have gooddomain.com, which is signed, and is not being modified by your policy at all. Now let’s imagine you have badthing.gooddomain.com which is not at a zone split boundary, and is just a regular non-delegation entry in gooddomain.com. If you add badthing.gooddomain.com specifically to your RPZ for modification, the server can’t deal with lying about just that entry, and the downstream validator will spot the lie, returning SERVFAIL to its downstream client(s).\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/alexa-why-are-you-broken/","title":"Alexa, why are you broken...?","description":"My Amazon Echo Dot and Show devices stopped working properly shortly after the advert and malware blocking was introduced","body":"For a bit of context, I have an Echo Show as a bedside alarm clock, and I also have an Echo Clock paired to an Echo Dot in the kitchen, primarily to visualise timers when I’m cooking.\nAfter a power cut late on Friday evening, the Echo Show came back on but displaying the wrong time (more or less an hour behind, but not exactly). If you asked “Alexa, what time is it?”, the correct time would be spoken, despite the wrong time still being displayed. Weird.\nSo, I got up, late, of course, and went to make breakfast. The Echo Clock in the kitchen is now also displaying the wrong time, but not the same wrong time that the Echo Show is displaying. Again, asking the paired Echo Dot for the time results in it speaking the correct time. Weirder and weirder.\nI tried a bunch of things with the clock, reset and re-pair, checking the location and timezone settings for the devices in the app, but nothing resolved the incorrect time. I also noticed that the clock wasn’t displaying timers properly, and the Echo Dot had stopped announcing the end of timers.\nI’d checked whether the Echo devices were unable to contact some central cloud service at Amazon with a quick check of twitter and some down detectors, and they didn’t seem to indicate widespread problems, and then it struck me: I wondered if my RPZ had picked up one or more entries that was causing this?\nFor some context at this point, I’ve been trialling the Energized Protection “Ultimate” in RPZ format. More about that soon in another post.\nI got the MAC addresses for the Echo units from the Alexa app, grabbed the assigned IPs from the firewall and then looked in the RPZ logs to see if anything was being blocked for those client IPs – BINGO!\nThe log entries all start at the time of the power outage, and the TTL on (at least) fireoscaptiveportal.com is 60, and so I wonder if the Echo devices resolve the IP and then continue to use the resolved IP, ignoring the TTL?\nThere were four domains continually being blocked for the two devices:\n\nfireoscaptiveportal.com\nmas-sdk.amazon.com\nprod.amazoncrl.com\nunagi-na.amazon.com\n\nI added whitelist entries to the RPZ for those, and immediatly could hear the Echo Dot in the kitchen announcing something … it was a timer from a few days before. As I stopped one, it would tell me about another, until it seemed to get very confused, resulting in a power off/power on.\nThe RPZ entries I added were:\n\nBoth the Echo Show and Echo Clock immediately corrected their displayed time, and timers were both displayed correctly and announced at the end correctly.\nI’ve subsequently added fixed IP leases in the firewall’s DHCP config for the Echo devices, and added client-ip whitelisting for them in the RPZ.\n\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/automatic-key-rolling/","title":"Automatic Key Rolls","description":"BIND9 has introduced dnssec-policy which makes it easier to sign your zones and roll your keys","body":"I recently moved my test domains onto a separate DNS master so that I could more freely tinker with these domains without risk to my regular stable domains.\nI use catalog zones (maybe this is for another post!) to distribute the zones to save myself the bother of having to configure all the slaves, particularly since I’ve recently started spinning up an experimental anycast network of virtual machines, and so I added a second catalog to my slave servers, and away we went.\nI’m a keen user of debian, and so I built the test master on ‘sid’ so I could run bleeding edge.\nThe benefit, primarily, was that whereas Debian 10 gets BIND 9.11.5, sid gets 9.16.8 (at the time of writing) as well as a newer version of openssl, meaning I could sign a zone with algorithm 16, ED448. DS digest algorithm 4 (SHA-384) is also supported.\nThe biggy for me, though, and the main driver for having the newer version of BIND, was dnssec-policy; getting BIND to automatically roll your keys.\nI’m not aware of an ability in this version to support either a hook to run a script to interact with your registrar to update a DS record when your KSK rolls, nor an ability to automate CDS or CDNSKEY, but they’ll be coming at some point in the future.\nI decided to test this by auto-rolling my ZSK, so, I added the following to /etc/bind/named.conf.options :\n\nYou don’t need to create initial keys or anything; BIND will do all the key juggling automatically, and will store them wherever you set key-directory to in your options section.\nIt doesn’t matter how you add your domains; I use rndc addzone as I have scripts that automate this as well as adding the zone to the catalog (again, that’ll be in another post at some point), the key thing being you just specify the policy in the zone. I’m also using inline-signing; whether you do will depend on your setup. Here’s a sample:\n\n…and as if by magic, rndc reconfig, and the keyfiles are created, and the zone signed.\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/more-anycast-dns/","title":"More Anycast DNS","description":"Anycasting DNS to the internet","body":"aka “how to do BGP with Vultr using ExaBGP”\nHaving previously written about locally anycasting services within my home network, I recently decided to run an experiment anycasting a prefix on the internet.\nI’ve used ExaBGP before, and so it was a no brainer to use it again.\nFor anycasted services, it offers a couple of benefits;\n\nit’s small and lightweight\nit’s in many linux distros\nit can easily spawn a watchdog process that you can use to control your prefix advertisements\n\nI’ve been using Vultr for my authoritative DNS servers for a while, and so it was also a bit of a no brainer to use their services for this. I’m familiar with their UI, I already have an account, and even on the cheapest virtual machines, you can do BGP; you just need to send them a letter of authority (LOA) proving you own the address space you plan on advertising.\nI have my own IPv6 PI address space from RIPE, courtesy of a friendly sponsoring LIR, as well as my own ASN, so I was all set.\nThe other nice thing about Vultr is that the BGP session is the same peer IP at the Vultr end regardless of which of their datacentres you choose to spin things up in which means your automation to configure things is easier.\nVultr insist on a BGP session password, and the first problem I ran into turns out to be related to this, and so part of the reason for writing this is to help out anyone that also runs into this problem.\nI went down a bit of a rabbit hole thinking the problem was to do with multihop BGP (Vultr’s sessions are multihop) and wondered if I needed to be setting the TTL on the outbound packets. This turned out not to be the case, but I left the settings in place anyway.\nEither in a template or neighbor configuration, depending on the complexity of your needs, you’re looking for  outgoing-ttl 2; and incoming-ttl 2;\nI installed BIRD and used one of Vultr’s canned configs for the virtual machine in question, and this worked like a charm, so this steered me in the direction of the problem being in my configuration of ExaBGP.\nBIRD would have done the job, but I’d have had to write a new watchdog, and the one I have for ExaBGP is tried and tested, tweaked to my needs, and works well, so I was keen to get ExaBGP working. It’s a complete re-write from the one I talk about in the earlier blog post, so maybe I’ll write a post on that soon…\nBy default, ExaBGP expects md5-password to be a base64 encoded string, and so if what you’ve specified is just the plain text string for the session, it won’t work. If you want to specify the plain text password in this parameter, you need to set md5-base64 to false.\nI’m using ansible to automate configuration, and so the template for my exabgp.conf looks like this:\n\nI don’t have any IPv4 prefixes to advertise, so you’d need to add the relevant bits to the above if you do.\nI had upgraded ExaBGP to version 4 as part of a distro upgrade on my internal resolvers, and rather than update the watchdog script, I opted for reverting ExaBGP’s setting instead. So in exabgp.env I altered the ack setting to false in the [exabgp.api] section.\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/bootable-usb-sticks/","title":"Bootable USB Sticks","description":"How to make bootable USB sticks","body":"I’m finally getting around to building a new ESX host to play with, and of course, nothing has an optical drive in it any more, so I need to make a bootable USB stick.\nMaybe I’ve not done this enough, maybe you’re rolling your eyes at me now, saying “really, you should know this”.\nI made the stick using the VMware instructions, but found that ESX didn’t boot. First off problems with it not finding some files (menu.c32, mboot.c32), and once I’d persuaded it to find them, loading -c...failed\nMuch googling ensued, before finding the gem that solved the problem. I was making the USB stick on a recent Ubuntu install, which was using syslinux v6.something. The useful gem was that anything after some early version of syslinux 4 was not going to work. I got syslinux 3.86 from kernel.org. That’s 32 bit, and so poked Ubuntu with a\n\nand then\n\nI then re-created the stick from scratch and hey presto, all worked well, so I’m putting this here, partly to remind me, and partly so google will index it and hopefully help you if you’re stuck!\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/validation/","title":"Key Validation","description":"The last in the series of posts on the root zone RFC5011 KSK keyroll","body":"Introduction\nFollowing on from&nbsp;my post&nbsp;about the new key being added to the zone, the required 30 days have passed and if your resolver is RFC5011 compliant, it should now trust the key.\nChecking…\nYou can check this as follows:\nBIND\n\nWe can see in the output above that the new key, keytag 19741, is now trusted.\nUnbound\n\nSimilarly, for Unbound, we can see above that the status is now VALID.\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/wiper-relay/","title":"Wiper Relay","description":"How to find and replace the wiper relay on a BMW E60 5-Series","body":"\n\t\n\t\tImportant\n\tNote that this article applies to the UK version when talking about the “passenger side” and is actually written because I couldn’t find an article helping with a UK layout.\n\n\nIntroduction\nRecently, the wipers have started to play up on my 2010 BMW E60 LCI.\nI booked it in to North Oxford BMW&nbsp;last Saturday morning, as it’d been there for services and I’m happy with the work and the customer care.\nOf course, sod’s law jumped in, and on the Friday night on the way home from work, the wipers worked fine. Grr.\nI took it along to BMW anyway, and thankfully the wipers played up for the technician, and the computer had logged helpful fault codes.\nThe motor was declared as fine, and a replacement relay was recommended.\nThey had none in stock, and so I ordered one to collect; I’m quite competent and can swap a relay.\nIn the mean time I checked the handbook; no mention of relays, just fuses. OK; off to google we go.\nRelay Location\nThe relays are in the “e-box” which is&nbsp;under one of the cabin air intake filters. A helpful Youtube video showed how to get to it.\nI picked the relay up this morning, and as the weather was nice, set about replacing it this afternoon.\nI undid the clips etc on the drivers side cabin air filter, and fiddled with the surround, before, eventually, the lower part came off, revealing…\n…no, not the e-box as expected…\nI was faced with the brake servo, which immediately made perfect sense; I immediately realised I’d watched a US video.\nSo I put it all back together, and started taking the passenger side apart.\nLet’s start again…\n\n\n\t\t\n\t\nImage of the passenger side engine bay before disassembly, note this is a UK model\n\nUndo that clip along the left edge of the cover, and there’s a clip you can undo with a 13mm socket on the lower right corner (as pictured). Lift it off.\nThen, you need to unclip the seal (pictured) - it just lifts off. There’s a clip on plastic cover in the middle below the windscreen, just visible in the left of the picture above; it just slides to the right and comes off.\nOnce that’s done, you can undo the screw (centre, bottom, next to the red battery terminal, in the picture above) and 3 more of the hex 13mm to be undone with the socket.\nYou can remove an odd little plastic cover that fits around the right hand bonnet stay with a bit of a wiggle, and then the cover comes off revealing…\n\n\n\t\t\n\t\nImage of the top of the e-box...\n\nUndo the alan screws holding the lid on, and…\n\n\n\t\t\n\t\n\n\t\t\n\t\nImages showing the insides of the e-box\n\nThat browny-beige relay is the wiper relay.\nThe invoice says it’s a B61.36.8.384.505.\nHope this helps someone.\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/a-new-key/","title":"A New Key...","description":"The next step of the RFC5011 keyroll of the root zone KSK","body":"Introduction\nFurther to&nbsp;my post on ICANN’s automated KSK testlab, ICANN generated a new key on the 19th, and added it to the test zone that we’re using, and we can see it below:\nThe New Key\n\nTroubleshooting\nKey 19741 is a new KSK in the zone.\nBIND9\nIf you look in managed-keys.bind (I’m running Debian, and so that’s in /var/cache/bind/) you’ll now see the new key is visible while BIND is observing the new key. RFC5011 defines the period that the resolver must observe the new key for as either at least two times the TTL of the keyset containing the new key, or 30 days; whichever is the longer.\nI’m cheating, slightly, and taking a look at managed-keys.bind from a different server, because my Debian box is running BIND 9.9.5, whereas I have access to a 9.11 box; you’ll see why below:\n\nOn my 9.9.5 server, I don’t have the helpful comments. We can see, helpfully, that the root key (19036), and our original testlab key (3934) are trusted. We can also see that the server observing key 19741 because the instead of trusted since we can see trust pending.\nUnbound\nIf you remember from the original post, whereas BIND keeps a track in managed-keys.bind, Unbound tracks the metadata in the external file we specified with auto-trust-anchor-file:. The file has been updated in a similar way to BIND’s:\n\nIn line 15, we see the original key (3934) with a status of VALID, whereas in line 22 we see the newly spotted key 19741 is ADDPEND.\nWhat’s next…?\nNow we wait; 30 days, and as long as the key is observed throughout, the key should become trusted at the end of this…\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/rolling-rolling-rolling/","title":"Rolling, Rolling, Rolling...","description":"The root zone is going to roll it's KSK following the RFC5011 defined process and method","body":"Introduction\nIn October 2017, ICANN are going to roll the key signing key in the root of the DNS.\nIf you’re not technical and don’t know what I just said, this post isn’t for you.\nIf, however, you run a validating recursive resolver, read on…\nIn October (the 11th to be exact), the key will roll and you’ll need to have done one of two things…\n\nUpdate your root trust anchor manually\nCheck your resolver is RFC5011 compliant.\n\nBut first, a little…\nBackground…\nSo you know how DNSSEC works…\n…you sign a zone. More specifically, you generate two keys, a key to sign the zone (ZSK), and a key to sign the keys (KSK). The zone gets bigger because for each record set, a signature is generated and added (RRSIG records). The public part of the keyset is also added to the zone (DNSKEY records). Some form of proof of non-existance is added (NSEC or NSEC3).\nNext, once the keys and signatures have made it to all of the nameservers for the zone, you generate a delegated signer record (DS) from the KSK, and you publish that in the parent. The parent then signs the DS record, and hey presto, your chain of trust is made.\nSo, where’s the DS record for the root…?\nTo make this chain of trust work, resolvers that want to validate the DNSSEC chain of trust need a starting point in the root…\nYour resolver has a trust anchor for the root. Depending on what you’re using for a resolver, this will either be the DS of the root KSK, or the public part of the KSK.\nYour resolver will have this built in, but then, if configured correctly, will use an automatic mechanism to keep that key up to date and roll it when required.\nRFC5011\nRFC5011 defines how a resolver can automatically update a trust anchor for a zone.\nSo that you can check whether your resolver will follow this process, ICANN have an automated testbed for the KSK roll, which I encourage you to look at.\nICANN’s Automated Test\nEach week, they create a new zone, and they sign it with a set of newly generated keys. Purposefully broken DS records are published in the parent zone, so that a normal validating resolver will SERVFAIL (because validation fails).\nBy adding a trust anchor to your resolver, the zone will validate.\nIf correctly configured, your resolver will now look for new key signing keys, and will observe them, and use them as per RFC5011.\nSo, lets take a look at this. Before I add a trust anchor, I can check that the zone doesn’t validate:\n\nWe can see in line 7, that we have a SERVFAIL response.\nThis server is running BIND. So, first we check that the server is configured manage keys using RFC5011:\n\nIf you’re just adding this, don’t forget to rndc reconfig.\nTrust Anchor\nNow, we need to add a trust anchor:\nBIND\n\nThis is added in your named.conf file.\nOnce again, don’t forget to rndc reconfig.\nUnbound\nIf you’re running Unbound, then you can add the DNSKEY or DS records to a file in a location that Unbound can read and write to (so, somewhere like /var/lib/unbound/ and then add a auto-trust-anchor-file line in the server: section of your unbound.conf file.\n\nNote; the file doesn’t look like this once you’ve told Unbound about it, as it uses the file to store metadata related to the RFC5011 process.\n\nAfter adding those, you’ll want to unbound-control reload to pick up the changes.\nTesting\n\nThis time, we can see that on line 7, we have a NOERROR response, and on line 8, we can see that we have ad in the flags.\nWhat’s next…\nNow, we wait. The next step is that ICANN’s automated test lab will generate and publish a new KSK into the zone on the 19th.\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/anycasting-dns/","title":"Anycasting DNS","description":"Anycasting DNS locally within my home network","body":"Introduction…\nI wanted to have a tinker with anycasting, and DNS seemed a sensible place to start, and easy to test and muck about with. So, I spun up a couple of DNS resolvers, and decided what my anycasted IP addresses would be. They need to be outside of the subnets I’m using on the rest of my network, as I want to route traffic to them. I’ve put the underlying machine’s unicast addresses in this subnet too, but you wouldn’t have to, depending on your set up.\nServers…\nThe nameservers are, essentially, identical to servers that’d deal with unicast traffic, except for the following changes. I’m using BIND, but it really doesn’t matter what you use.\nWe need to bind up the anycast addresses so that the O/S will deal with their traffic…\nIn my case, my anycasted addresses will be 10.1.53.1 and 10.1.53.2, and I’m using Debian, so my additions to /etc/network/interfaces are:\n\nWe need to stop the machine responding to ARP for these. Actually, we tell it to stop responding to ARP requests unless the interface the ARP arrives on matches the ARP’d for IP, so because we’ve bound them up to the loopback, we don’t want the machine to respond via eth0, for example, so I added the following to /etc/sysctl.conf:\n\nBGP &amp; Load Balancing…\nNow we need to advertise the anycast addresses to our router. In this case, we’ll use BGP to do this. To do that, we’ll use ExaBGP. Grab that and install it on the server, and then the config looks something like this. My router is 10.1.53.254, and my two nameservers live in 10.1.53.0/24\n\nI withdraw the routes from the outset, so that the watchdog will announce them upon successful testing.\nThe router’s BGP config looks like this (it’s JunOS):\n\nI’m going to equally load balance between the two servers, but you could set a localpref on each server, for example, and have server1 handle .1 primarily with server2 taking over in the event of failure, and vice versa.\nDon’t fall for JunOS’ misleading ‘per packet’ configuration item; this will, despite appearances, load balance per flow based on a hashing algorithm.\n\nMonitoring and Health…\nWe’ve included a watchdog in the ExaBGP config. Without this, clearly if the nameserver fails entirely, then the BGP session will be torn down, and the traffic directed to the other host. However, if the nameserver daemon fails, then the BGP session will remain, and traffic will be disrupted. Therefore, there’s a watchdog that’ll check that the nameserver daemon is listening, and will perform a lookup against it, announcing the anycast address(es) while it’s up, and withdrawing them in the event of failure. The watchdog looks like this:\n\n/etc/nameserver_watchdog.conf contains lines of the format ip.ad.dr.ess:domain.com.\nIt’ll announce the address in the event that a tcp connection succeeds as well as a DNS lookup that you’d expect the server should answer or be permitted to recurse for you. If the DNS daemon stops responding the watchdog will withdraw the routes; if the server fails, the BGP session will fail, and the route will be withdrawn anyway.\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/live-electricity-usage/","title":"Live Electricity Usage","description":"My set up to have live monitoring and graphing of my electricity usage","body":"\n\nLive Electricity Usage Graph\n\nSo, following on from my recent post on updating my Currentcost code I now have every update coming from the Currentcost unit popping directly into the database, every 6 seconds.\nI had become aware of the Highcharts charts javascript and when reading through their Live Charts Demo decided that this was just waiting to be played with.\nI now have essentially the same graph that they have in that demo, and the page HTML is very similar. The difference is obviously to call my code instead, and also a change to refresh every 6 seconds:\n\nMy AJAX call is also to some PHP, but mine looks up the latest data from the database with a simple SQL statement:\n\nI quickly realised that this would slap the database, and more so if more than one person was viewing the page. That would be unnecessary, and so I installed memcached and the php5 memcached module.\n\nEdit: Slight tweak to the code, as there was some obvious lazyness that I tidied up.\nEnjoy.\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/watt4-live-black-horse/","title":"Watt4 at The Black Horse","description":"Pictures of Watt4 playing live at a nearby pub","body":"Well, it’s been far too long since I enjoyed getting distracted from photography one evening by the live music from Watt4.\nFinally, however, I got around to finishing the processing and uploaded a selection of the images taken that night.\n\n\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/current-cost/","title":"CurrentCost Electricity Monitoring","description":"How to do cost effective electricity usage monitoring from CurrentCost devices","body":"For a while now, I’ve been using a Current Cost CC128 to monitor my electricity usage.\nI hacked together a munin plugin to monitor both the electricty usage, and as the unit is in my lounge and coughs up temperature, it also provides the temperature reading.\nFor a number of reasons, the munin plugin doesn’t read the device directly. Only one device can open the serial port at a time, and there were a couple of things I wanted to do with the data, so didn’t want them fighting. I had a small piece of code grab the XML once a minute and dump it in a file instead.\nTalking to a friend recently, they quite rightly pointed out (reminding me) that this method takes an instantaneous value for both the temperature and the electricity usage.\nWhilst not a problem for the former as it won’t vary greatly within the 5 minute polling periods, the electricity usage clearly will. Think about boiling a kettle; if you were (un)? lucky enough to time the kettle just right, you could be between polls and not record the increased usage at all.\nSo, I’ve modified the way in which I record the data and write the files for the various things including munin to use.\nAs you’ll probably be aware, the CC128 outputs it’s data once every 6 seconds, so there’s clearly more data to make use of than just the single reading every 5 minutes.\nI have a small daemon written in perl that listens on the serial port and every time it gets a full string of XML, it records the data to a database. Rather than re-write the munin plugin at the moment, along with anything else that uses the data file, I have a small bit of perl that runs from cron each minute that populates the expected XML into the file using averages of the values collected in the preceding 5 minute period. You could, of course, make this 95th percentile or whatever tickles your fancy.\nSo, the SQL table looks like this:\n\n\nI should credit http://larig.wordpress.com/2012/02/08/time-rounded-to-five-minutes-in-mysql/ for the sql to round time to the current 5 minute window.\nThe bit that writes the XML file looks like this:\n\n…and lastly, the bit that grabs the stuff and chucks it into the database looks like this (and also writes a backward compatible version of the XML file with the instantaneous values…:\n\nI restart that from cron periodically, incase it dies. If it’s already running, it simply exits silently. It’s a quick hacky way to make it restart in the event something horrible happens. I did at least put the XML parse in an eval in case it barfs :-)\nAs usual, this blog post is as much for my personal notes of how I did it than anything else. Some of the code is awful, but it’s not life or death and so I’ve often been lazy with it. There’s no doubt this could be done prettier and less hacky if I had more time :-)\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/temperature-monitoring/","title":"Cost Effective Temperature Monitoring","description":"Temperature monitoring with DS18B20 1-wire bus sensors and a raspberry pi","body":"\n\nTemperature Graph produced by Munin\n\nI’d been thinking about some temperature monitoring at home for a while, particularly in the nursery, so I can see if it’s getting too hot or cold in there while I’m asleep.\nWhen my Raspberry Pi arrived, I decided to put it to work.\nAfter quite a bit of googling &amp; reading, I found that the one wire file system stuff was easy to install, and had been done by others (See references below).\nWhilst it’s possible to go and buy the individual components, and I’m quite capable with a soldering iron, I have to admit to taking the lazy route and purchased some bits from Sheepwalk Electronics\nI wanted to monitor temperature in the garage, outside, the loft, the front bedroom, and the small bedroom on the back of the house (currently, the nursery).\nI also monitor temperature in the lounge, but that comes from the Current Cost unit that I also monitor electricity usage with.\nSo, I’d need a host adaptor, and a sensor for each of the places to be monitored.\nThe Sheepwalk sensor modules are made up, or kit form, and include RJ45 sockets to ease putting together a network with standard cat5 cable. The sensors can either be parasitic and pull power from the bus, or can have power supplied. I’ve set mine up as parasitic, and it’s working OK so far.\nOne wire is best suited to a linear network of devices strung together, but seems quite robust, and is working in a star arrangement.\nI purchased Sheepwalk’s SWE2+SWE0 pack which comes with an SWE2 (basically, a sensor plus 6 RJ45 sockets) and four SWE0 (a basic sensor on the end of 2m of cat5). I also purchased a USB host adaptor, and the RJ11 to RJ45 cable, although this would be simple to make up. Lastly, I picked up some RJ45 couplers, as they’re a good price and I’d be sure to need some!\nI installed Raspbian (the official/recommended linux distro for the Raspberry Pi) on the SD card and booted the Raspberry. I gave it a static IP on my network so I could probe it from the Munin instance later on without fear of the IP changing.\nAfter inserting the USB adaptor, I installed OWFS\n\nThere’s no init script for owfs itself, only for the supporting services, but Neil Baldwin’s page that I’d been reading had one, which saved me the hassle of writing one.\nWith that started, I plugged in the RJ11 to RJ45 lead, and plugged the SWE2 into the end.\nA fresh ls -al /mnt/1wire/ showed the new sensor, and\n\nI tested the rest of the sensors, before hooking them up where I wanted them. I was a little concerned that the cable length might be a problem to the furthest sensor - the sensor in the loft is about 25m of cable from the SWE2, and is strung off the SWE1 that provides the “Front Bedroom” sensor.\nLastly, I hacked up a munin plugin. All I needed was the munin-node package (apt-get install munin-node) as my remote munin instance would be collecting the data and producing the graphs (it’s already doing things like the already mentioned temperature in the lounge, electricity usage, and more mundane things like the SNR &amp; sync rate of my ADSL line from my router).\nSo, in summary:\n\nRaspberry Pi + SD Card + Raspbian + OWFS + Munin-Node\n\nUSB Host Adaptor + RJ11 to RJ45 Lead\n\nSWE2 (Garage)\n\nSWE0 (Garage Outside)\nSWE1 (Front Bedroom)\n\nSWE0 (Loft)\n\n\nSWE0 (Small Bedroom)\n\n\n\n\n\n\n\nCode Snippets\nFirst up is the munin plugin. I took a chunk of inspiration from http://err.no/personal/blog/2010/Nov/02 but the perl OWNet module was playing up for me, and given it was 10pm, I took the lazy route and hacked it about to just look around the file system. You can tell how lazy I was, because you can see I used glob() instead of opendir() etc. I’m not proud of it, but it’s working.\nYou’d need to drop this either directly in /etc/munin/plugins or someplace else and symlink it to there. I did the latter.\n\nNext up, the aliases file /etc/owfs-aliases - again, it was late, owfs aliases didn’t appear to be working quite how I was expecting, and so I hacked the plugin to use this file:\n\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/ipv6-privacy-extensions/","title":"IPv6 Privacy Extensions","description":"What are IPv6 Privacy Extensions?","body":"Introduction\nIf you’ve switched on IPv6, whether via a tunnel, or natively, your machine is likely to have stepped out from behind NAT and now has a globally routable address.\nWhilst NAT is not security, and inbound connections to your machine are likely behind a firewall, that doesn’t change one thing: in many cases your IPv6 address is made up automatically by auto-configuration.\nIt’s made up of two parts, the last 64 bits are put together partly from the MAC address of your network interface, and the rest comes from the network prefix.\nThis makes your machine globally identifiable, and therefore, trackable by third parties, such as web sites.\nTo this end, RFC3041, superseded by RFC4941 defines privacy extensions.\nWhen enabled, your machine still has the auto-configuration address, but now also has a randomised additional address that changes periodically and is used for outbound connections.\nOn many linux distributions this is disabled by default. Windows XP is the same. Windows Vista enables it by default, and I believe newer versions of Windows also enable it by default.\nI don’t have access to any Windows Server installations with IPv6, so I’m unsure if the server editions do this too.\nI mention servers, as you probably don’t want this on a server. Imagine, for example, a mail server making an outbound connection from a random and short lived IP address. It’s unlikely to have a valid PTR, for example, and many, not all I grant you, but many MTAs will not like that.\nEnabling it on Linux\nEnabling it on linux distributions is quite straight forward:\nas root:\necho 2 &gt; /proc/sys/net/ipv6/conf/all/use_tempaddr\nYou can automate this at boot in the normal way; edit /etc/sysctl.conf and add the line:\nnet.ipv6.conf.all.use_tempaddr=2\nYou can swap all for a specific IPv6 enabled interface, such as eth0 if you require.\nEnabling it on OS X\nOS X has it disabled by default. I believe you can add to, or create /etc/sysctl.conf with the following:\nnet.ipv6.conf.all.use_tempaddr=1\nDon’t quote me on that last one; I’ve not tested it!\n[edit 21/Mar/2017]: MacOS Sierra seems to have it enabled by default…\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/dnssec-bind-configuration-summary/","title":"DNSSEC BIND9 Configuration Summary and Cool Stuff","description":"How to DNSSEC sign your DNS zones with BIND9","body":"Introduction\nWith the recent signing of the root, I’ve discovered a sudden interest in DNSSEC, and decided to have a go myself to aid my understanding of it.\nThis article is written as an aid-memoir to me, and summary of the bits I’ve read. Of course, I’ve provided links to the whole blog entries I found the information in, in case you want to read more than I’ve written.\nWhilst the root is signed, only certain TLDs are signed, and so if you want the full chain of trust experience, you want a domain with a signed TLD.\nAt the moment, .uk is signed, but .co.uk etc are not, so that rules them out. .net is scheduled for around Nov 2010, and .com sometime around March 2011.\n.org, however, is already signed, and so I thought I’d grab one to play with.\nNot All Registrars Are Equal\nI used my regular registrar, and registered karldyson.org to go with my collection of .com and .co.uk versions.\nThis was my first sticking point, because their upstream (Tucows) aren’t accredited for DNSSEC yet (and, it would appear, have no plans on doing so).\nI’d need my domain to be with a registrar that is accredited.\nMy registrar helpfully supplied me with a list of registrars that are, so I could choose one and either register a domain there, or move my new one.\nI registered another .org to add to another set, this time with GoDaddy. They’re on the list.\nSigning The Zone\nI had told GoDaddy that I wanted to use my own nameservers during sign up, and so after creating a regular zonefile for bind, I had a look through the blog entry I found at http://clayshek.wordpress.com/2009/01/13/enabling-dnssec-on-bind/\nEssentially, the steps are (all completed whilst IN the zonefile directory):\n\nGenerate a zone signing key (ZSK) :\n\n\ndnssec-keygen -a RSASHA1 -b 1024 -n ZONE example.org\n\n\nGenerate a key signing key:\n\n\ndnssec-keygen -a RSASHA1 -b 2048 -n ZONE -f KSK example.org\n\n\nConcatenate the created public keys into the zone file:\n\n\ncat Kexample.org+*.key &gt;&gt; example.org\n\n\nSign any child zones first:\n\n\ndnssec-signzone -N INCREMENT child.example.org\n\n\nConcatenate the DS records for the child into the parent zone:\n\n\ncat dsset-child.example.org &gt;&gt; example.org\n\n\nSign the zone:\n\n\ndnssec-signzone -N INCREMENT example.org\n\nGenerating the ZSK and KSK took ages on my Atom 330 dedicated server, and so I can recommend a good book, or some other talk while you wait for this to finish!\nLike the child zone signing, you will get DS records for the parent zone. These need to be supplied to your registrar to maintain the chain of trust. GoDaddy has a nice interface for submitting these, you just need to know what the different bits of the DS records are. They’re detailed in RFC4034 but to save you some time, and sanity….\nYour DS Records\nexample.org. 86400 IN DS 60485 5 1 2BB183AF5F22588179A53B0A98631FAD1A292118\nThe first four text fields specify the:\n\nname example.org.\nTTL 86400\nClass IN\nRR type&nbsp;DS\n\nValue 60485 is the key tag for the corresponding&nbsp;example.org. DNSKEY RR\nValue 5 denotes the algorithm&nbsp;used by this example.org. DNSKEY RR.\nValue 1 is the&nbsp;algorithm used to construct the digest.\nThe rest of the RDATA&nbsp;text is the digest in hexadecimal.\nYour Caching Resolver\nYour caching resolver will need DNSSEC enabled for queries. I added the following to my bind server’s options section:\n\nYour System Resolver\nWith your local system pointed at your caching resolver, it would appear you’ll need EDNS0 enabled. This is achieved by adding the following option to your /etc/resolv.conf.\n\nThis appears to be supported on newer versions of libresolv - my Debian 5 system doesn’t appear to support it, whereas my Ubuntu 10.04 system does.\nSo, on to the cool stuff… SSH\n..and so, at last, on to the cool stuff.\nGiven you can now cryptographically trust DNS, you can do something interesting. Rather than need to verify all the SSH fingerprints, you can store them in DNS and have your SSH client automagically verify that all is well.\nI followed a set of instructions I found at http://blog.exanames.com/2009/06/one-more-thing-to-do-with-dnssec-ssh.html, and as before, here’s a summary.\nRun the following two comands on each host you’d like to generate fingerprints for:\n\nThis will generate two SSHFP records that you will need to include in the zonefile, then you can re-sign and re-publish the zone.\nIn my case, the records generated were for .co.uk variants of the hostname, but I found no problems changing them to .org\nYou’ll then need to persuade SSH to perform verification using DNS. I did this by adding the relevant option to /etc/ssh/ssh_config\n\nThere, you’re done. You should now be able to ssh to the host(s) concerned without needing to manually verify the fingerprints.\n"},{"url":"https://karldyson.github.io/karld-blog-website/posts/tcp-53-isnt-just-for-axfr/","title":"tcp/53 isn't just for AXFR","description":"You're not just opening UDP/53 for DNS, are you...?","body":"The internet has a defined set of rules known as RFCs. They work together to make sure that all&nbsp;participants&nbsp;of the internet community are working in the same way, and that the things they do as part of that community will work with, and interact correctly with the things that others do.\nRFC1035 section 4.2.1 (UDP Transport) states:\n\nMessages carried by UDP are restricted to 512 bytes (not counting the IP or UDP headers). Longer messages are truncated and the TC bit is set in the header.\n\nYou then fall back to TCP and repeat your query to get the full response.\nYes, AXFR queries are TCP, but TCP isn’t exclusively AXFR!\nAssuming that, because you won’t be doing any transfers and therefore don’t need to allow tcp/53, is wrong, and will invariably involve you having issues with some service or other, due to not getting the correct information from DNS.\nI’ll give you an example:\nYou use a service that, for whatever reason, decides that as a basic form of load balancing, to use multiple A records for the ‘name’ you’ve queried. So, you ask for www.example.com and, rather than give you back a single A record, they give you one for each of their servers. This could easily become longer than 512 bytes, the answer will be truncated, and you should repeat your request using TCP. Your resolver knows this, and will automatically do it.\nIf you’ve blocked tcp/53 on your firewall, it’ll fail. You’ll sit, staring at your computer, thinking that www.example.com has fallen over, failed in some way, when they have not. It’s not their fault that you’ve not followed the rules.\nIt’s not just website related records either, email related records (MX, or the TXT for DKIM, DomainKeys or SPF) are other great examples of this…. block tcp/53 from your mail server, and you could quite easily find yourself not receiving email from some senders.\n[edited later to add the following]\nFurther reading (thanks to Duncan for the pointer) of RFC1123 section 6.1.3.2 (Transport Protocols) states:\n\nDNS resolvers and recursive servers MUST support UDP, and\nSHOULD support TCP, for sending (non-zone-transfer) queries.\nSpecifically, a DNS resolver or server that is sending a\nnon-zone-transfer query MUST send a UDP query first. If the\nAnswer section of the response is truncated and if the\nrequester supports TCP, it SHOULD try the query again using\nTCP.\n\nI guess I’m a little disappointed to see “SHOULD” instead of “MUST”, but given the document was written in 1989, I think today, it should be read as “SHOULD, if you want it to work”. It does go on to say:\n\nWhether it is possible to use a truncated answer depends on the application. A mailer must not use a truncated MX response, since this could lead to mail loops.\n\n"},{"url":"https://karldyson.github.io/karld-blog-website/","title":"Karl's Little World","description":"","body":"\n\t\n\n\n\nKarl’s Little World\nI’ve written a few articles through 2025 and have been increasingly dissatisfied with the themes available on Wordpress’ platform as a “Personal” subscriber.\nI was never quite happy with selecting a theme that gave me both what I wanted for individual articles, nor the occasional gallery for those all too infrequent opportunities to play with the camera.\nWhilst reading up on something recently, I came across the DuckQuill theme for Zola. This led me to tinkering with the templates.\nI don’t think I’ve quite got 100% to grips with it yet, but it seems to be giving me enough flexibility, whilst still making everyday articles pretty straight forward to just write (they’re basically Markdown).\nGiven my Wordpress subscription renews (or (spoiler), not, as it happens) in February and with a little time off over Christmas, I decided to migrate documents from Wordpress, picking the ones that would challenge the ability to tweak the templates to my satisfaction first.\nSo here we are. I hope you like it, and I hope you find it easy on the eye.\nI’ll spin up a contacts page soon enough, but in the mean time, email me, explore the discussions section of the repository, or toot me.\nThanks,\nKarl\n"},{"url":"https://karldyson.github.io/karld-blog-website/latest-posts/","title":"Latest Posts","description":"","body":""},{"url":"https://karldyson.github.io/karld-blog-website/archive/","title":"Archive","description":"","body":""},{"url":"https://karldyson.github.io/karld-blog-website/about/","title":"About Me","description":"","body":"Hello and thanks for taking the time to look around.\n\nMe\nFirst and foremost I’m a father.\nAfter that, in no particular order, I’m a DNS geek; an Enterprise Architect; a Motorsport UK Incident Officer; a Motorsport UK Clerk of the Course; and a photographer.\n\nMotorsport\nI’ve always been interested in motorsport. I’d seen those folks in orange on the telly and had occasionally wondered how you got involved in that.\nThen, in about 1999 a friend and I were chatting at a car club meeting and he said he was going to a marshal training day the following weekend.\nIt never occurred to me that you could just turn up, volunteer, and be a marshal.\nSo I started going. I enjoyed it. I made a lot of friends.\nAfter a number of years “on the bank”, I started to get more involved in the running of the race days, working in Race Control in a variety of roles, and eventually completed the training to become a Clerk of the Course for circuit racing events.\nI completed the first phase of that, with my license upgraded to “Deputy” status, and from the start of the 2025 season I’ve added the Assistant (training/probationary) category for “Speed” events which for me is predominantly allowing me to rediscover my love of hill climbs.\n\nCareer\nI’m very lucky to have a job that is mostly like getting paid to do a hobby. Does it have its days? Of course. But I’ve never woken up disliking the fact that it’s a work day. Occasionally (ok, more often than that) I wake up disliking the fact that Oxford’s traffic is between me and the office!\nI have a background in the design and operations of large scale critical infrastructure and services, mostly involving DNS, DNSSEC, BGP, etc.\nI’m not a software developer, but I can get by in perl, python, and a bunch of similar things. I’m currently kinda learning Go.\n\nPhotographer\nI enjoy spending time behind the camera. However, I often pick officiating at Motorsport events instead of photograhing them, for example.\nYears ago I learnt from research, enthusiastic friends and a pro I knew, to spend as much as you can afford to on the glass, the body should be mostly a secondary consideration. It’s not quite that simple, but you get the idea.\nNo amount of money spent on a great body can make light appear at the back of poor glass.\nSo, I started with a Canon 20D, I picked up a 24-70L f/2.8 with the body. A friend was upgrading and sold me a used but great condition 70-200L IS f/2.8 with a 1.4x teleconverter and I was set.\nMore recently I was very luck to overhear a friend at the office talking about having just bought themselves a new DSLR and I enquired if what they were replacing and were they selling… TL;DR is that I now have a Canon 5DMkIII in excellent condition, and they threw in the 50mm f/1.8 with it because they’d jumped ship to Sony and had no use for the 50mm any more.\n\nWhat you’ll find here\nMostly what you’ll find here is stuff I think I should write down. This usually happens when I’ve either searched for something and couldn’t easily find it, or I just fancy writing a post on something I’ve done or am doing.\nIf I’ve found a few minutes to tinker with the camera, there may be pictures to look at too!\n"}]